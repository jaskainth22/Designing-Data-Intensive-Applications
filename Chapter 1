Reliable, Scalable and Maintenable Applications

What a Data intensive Application have? What are the building blocks?
 - Store data (Databases)
 - Remember results of expensive operations (Caches)
 - Allow search by keyword or filter (search indexs)
 - Send message to another process async (stream processsing)
 - Crunch large amount of data (batch processing)

Data Systems - Databses, Caches, Message queues. All of these are Data systems. All store data, but the way they store it, the duration for which they store it, differentiate them.
We can use a message queue (Kafka) for database pruposes as well as they provide database like durability.
But its a good idea to use specific data system for specific requirement.
Suppose you have a DB, Cache and a Search engine. Now its the duty of application code to keep them all in sync.


Three Important Pillars for designing Data intensive apps
  - Reliability - The system should continue to work correctly even in the face of adversity (hardware or software failures or human errors).
  - Scalabilty - As the system grows (in data volume, traffic volume or complexity), there should be reasonable way of dealing with that growth.
  - Maintainabilty - With time as we maintain current behaviour and add new features, people should be able to work on the system productively.

Reliability
System should give the desired output for a given input. It should be able to handle user mistakes. 
Our system should be fault tolerant. What is fault tolerant? It should be able to gracefully handles faults. 
There is a difference between fault and failure. Failure is your system is not responding at all. Faults are things that can go wrong (wrong input, service failure etc.)
Due to faults our system should not fail. That is why we need fault tolerance in our systems. 

Hardware Faults - When faults occurs due to hardware issues. Power outage, RAM fault, Hard disk crash. Hardware faults are not stoppable. What can we do?
We can add backup generators to prevent power outage. We can have data duplicated to other disks for Hard disk crash. etc. etc.

We need redundancy of hardware components (RAM, disk etc.) We don't need redundancy of full machines. As it is very rare to get a full machine down. However for machines
where downtime should be very low, we can have Full redundant machines as well. 

Software Faults - These can be catastrophic. One software failure can lead to multilpe other software failure as well. One bad input to a request may lead to softwarfe failure.
One small bug in code/service can lead to downtime of other services as well that are dependengt on this.

Human Faults - We cannot do anything in this. Humans tend to make mistakes. What can we do? Have sandbox environment for testing, Have Clean UI, use abstarction, minimal user input,
automate everything. 


How important is Reliabilty?
Reliabilty is very importnant. We can lose productivuity, Business, Have law suits if our system is not reliable.
Suppose Google Photos gets corrupted all your data is gone, You have no backups. Then what?  So our systems should be reliable. They should be able to handle faults gravefully
and faults should not lead to failures.



Scalability
If a system is reliable today, that doesn't mean it will be reliable in future as well. With increase load systems fail. So scalabiluty means as the load increase our system
should be able to gracefully handle that load.

Describing load - No. of req per second, No of simulataneious person in a chat room, No. of viewers on a live stream, No. of read:write on a DB

Lets take an example of twitter
1. Write tweets (Max. 12k/sec)
2. Read tweets (Max. 300k/sec)

Main challende is read tweets. So twitter has 2 approaches, At the time of writing tweet, it can just save the tweet and then when user asks for home feed, we search for all the
tweets and then select the users followed bu this user and filter tweets based on that. This is very slow. First version of twitter followed this, but this became a issue
quickly. So they need a better version.
So they created a cache feed for each user ahead of time. If a user make a tweet we push this to all its followers cache. So when user loads home feed, it loads without any computing,
as it is done already.

But there is a downside for this as well. Now writing tweets is time consuimg, as we need to push this in all the feeds of followers.
And if a user have 30M followers, we need to do 30M writes. 
So later twitter followed a hybrid approach. For users with less followers they fanout the tweets (appraich 2). For user with large followers, they user approach 1.
At the time of loading if a user follows a celeb, we will fetch its tweets and merge woth the users cache feed.

From this we can learn that every system have different kind of load. Twitters load are on write/reads per second. Another system may have some other kind of load.


Describing performance - Once load is defined, we need to investigate what happens when load is increased.



Latency and Response time - These two are ofte user synonymously, but they are different thing. Response time is after what time a client sees a response for a request. It includes 
network delayes, queue delays etc. Latency is the time when the request is waiting to be handled. Durting which it is latent, awaiting service.



Percentile time - How much time it took to get the response.
p50 for 200ms, means 50% user get the result back in less than 200 ms and for other 50% it took more time.
p95 for 1.5s, means 95% user get the results back in less than 1.5 ms and for other 5% it takes more than 1.5s.

High percentile of response time, also known as tail latencies. These affect user experience.  If p99.99 response time is 10s, this is bad, we need to turn the time down from 10s to somewhere 3s.
It is very hard to do this and sometimes its not worth the efforts and money.

Queueying delays are the most culprit for increase in response time. If a request is holding the resources and other requests are waiting, it increase the response time.
Even though the other requests maybe faster but still the overall time is increased.

When several backend calls are needed to serve a request, a single slow backend request slow down the entireend user request.



















