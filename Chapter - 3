Storage and Retreival

In this chapter we will see how data is stored inside a DB.

Simplest form of DB.
DB_GET(key) returns value
DB_SET(key, value)

So DB_SET will set the key-value at the end of file.
It will not deted the previous entries of the same key.
SET operation is quite fast O(1).

DB_GET will search for the last occurence of this key and returns the corresponding value.
This operation is quite slow. We may need to search the entrie file. O(n).

We can optimize this search operation using indexes.

Index is an addiitonal data strcuture. The general idea behind them is to keep some addiitonal metadata on the side, which acts as a 
signpost and helps you to locate the data you want. An index is an additional strcuture that is derived from the primary data. 
Many DBs allows us to add and remove index, this doesn;t affect the content in DB, it only affect the performance of queries.
The read operation will become fast by using indexes but the write operations will become slow.
So its a tradeoff. 

=========================================
29/03/2025


Hash Indexes:
We know what a hash table is. For a given key we can look for a value in a hash table in O(1) time.
So for indexing Key-Value store, We can use HashTable for indexing.
Take the example where we are storing the Key-Value in a file. Each write is done at the end of a file.

Now For each write do a entry of that key in Hash Table. And for that hash key the value will be the offset/memory pointer where that data is stored. 

For Ex.

Key - Value Store
Cat - 1
Mouse - 2
Dog - 3
Cat - 4
Dog - 5

Corresponding Hash Table
Cat - 1, 4 (These are basically the offsets/memory pointers from where we can directly fetch this data)
Mouse - 2
Dog - 3,5

If you want to get the latest value, return the last value in the list
This is the scenario where we want to keep all the entries of a key in hash table.

Suppose if you only want the latest value and the past values are redundant to you.
We need not store the complete list. For ex. we are just storing the counter of how many times a key is accessed.

Cat - 4
Mouse - 2
Dog - 5

Since we are only writing to a disk, We can run out of disk space. How do we prevent that?
So don't write in a single file, When one file reach a certain size, start writing in other files.
Then perform a compaction on all the files. Meaning merge the file and keep only the latest entry.

This compaction can be an async process. Once the compaction is done we are safe to delete the old files and just read from the new one.

Each segment (file) has its own hashmap. So when we need to look for a key, we first check the most recent segment hashmap, then the second most recent and so on.
Since compaction and merging keep the no. of segments on lower side.

BTW this hashmap is in memory. Using system RAM.
Bitcask is a very popular Key-Value store that works well with just this basic approach.

1. Deleting a record - For deleting a record we add a flag in the file/segment, When we do merge, We discard the previous key from hastable and the new segment entry.
2. Crash recovery - If the server restart the in memory data store is lost. We can normally traverse the all segemnts and generate the hashmaps again but that is very slow. Instead we can keep the snapshots of the hashmaps and store them in datadisk.
3. Partially written records - If the system crashes midway, Bitcask have checksums to detect that and rollback.
4. Concurrency control - For writing we will have a single write thread but for read we can have multiple. So write will happen in a sequential manner but for reading multiple threads can access at the same time.

An append only design seems like a waste. Why don;t we update the values instead of keep appending and then doing merging and maintaining seperate hashmaps for each segment. 
Well there are advantages.
1. Appending and segment merging are sequential write operations, which are much faster than random writes.
2. Concurrency and crash recovery are much simpler if segment files are append only. Bcz suppose during a write a crash happened. If we have random writes we can be in a state where we are in between the old and new data.
3. Merging old files avoids the data fragmentation problem.

Well there are some issues as well in using HashMap index
1. Since its in memory we are limited. Once the RAM is full, we cannot do anything. We can use a in memory hashMap but that is not fast enough.
2. Range queries are not efficient. We cannot look up all the keys between certain size (1000 - 9000). We need to go over all the keys one by one.


=================================
30/March/2025

SS Tables (Sorted string table)
In the normal log segents with hash indexing, we were just appending the key-value pairs at the end of the file. The older values of the key takes precedence. Otherwise it doesn;t matter how the other keys are arranged.

If while writing we sort the key value pairs based on the key, they are called SS tables. We also require that each key only appears once in the merges segment, that is already ensured by the compaction processs.

How is this helpful over the normal log segments with hash indexing?

1. Merging segment is very easy. We will use the concept of merge sort. We will take the most recent and the second most recent segment and merge them. The merged segment is also sorted.
What if 2 segments have the same key?
Its simple, choose the key-value which is most recent (i.e. from the latest segment).

2. We not don't need to index each key. Why? We can index some keys and then from those keys we can get to our target key since the file is sorted.

For ex. 
Index
HA1 - 0x7489
HA5 - 0x8780
HA9 - 0xiue2

Suppose you need to find HA7, Since you know the offset of HA5 and HA9, just scan between these keys in the SS Table.

This prevent memory exhausation since very few keys are indexed.

3. Since for read requests we need to scan over multiple keys, We can divide the SS (sorted segment) tables in multiple blocks.

For ex. All the keys from HA1-HA4 in Block1. All the keys from HA5-HA8 in other block.
We can compress these blocks and save them on disk. Now the index will point to the offset of these blocks.
This prevents space and network I/O.


Constructing and Maintaining SS Tables
How can we get the Key-Value pairs to be in sorted order?
maintaining a sorted structure in disk is possible (B-Tress)
maintaining a sorted structure in memory is very easy.
We can use a Tree data structure like Red Black trees or AVL trees. In these trees we can insert the data in any order but we will read them in sorted order.

******  VERY IMPORTANT START  *********
We can make our storage engine work as follows:
1. Have an In memory Tree Data structure like AVL tree, Write the data to this tree. Since this is in memory we can call it memtable.
2. When the memtable reaches a certain size write the data on disk as a SS table file. This new SS table becomes the most recent segment.
3. For reading we will first try to find the key in the memtable, then the most recent segment, then the second most.
4. From time to time run a merging process to merge these segments. With this we can remove the deleted keys and overwritten keys.


This suffers from only one problem, In case of a crash the data that is present in memtable will be lost, as it is still not written to SS table.
To remove this problem, just have a log file which will append all the data in the end. It will just write the data in memtable.
Once the memtable data is written to SS table, we can discard this log. This is just for crash purposes.

******  VERY IMPORTANT END  *********

Above algorithm is used in LevelDb and RockDB. Storage engines in Cassandra and HBase are also inspired by this. These type of storage engines are called LSM engines (Log structured merge tree).

Performance optimizations:
When a key does not exist, we need a lot of read operations.
We first need to check the Memtable, then the various segments. Only then we can be sure that this key does not exist.
This is very long operation. To optimize this we can use Bloom Filters. (It is an in memory datastore that can efficiently tell if a key exist in DB or not.)




B-Trees
Most relational DBs uses B-Trees for indexing. How is it different from the log based indexing?
Well in log based indexing we have a segments of variable sizes, with each segment having a index (hash table). 
But in Btrees we have a fixed size block. In these blocks we store key value pairs.
One block will have ref to other blocks. The value for a particular key will be at the leaf node.

Root Block:
ref, 100 - ref, 150 - ref, 200 - ref |  This block will lead us to data between keys 100 <= key < 200

ref , 111 - ref, 127 - ref, 140- ref | So the first ref in the root block lead us to this

This way we need to traverse and we will get to the jey we are looking for.
O(Logn) for search/ Update.
For adding new key search the particualr block, if there is space insert the key.
If not divide that block into 2 full sized block update the ref in parent block and do addition.

B-Trees are quite reliable and has been time proven.

Most fundamental difference between B-Trees and Log based LSM Trees is that B-Trees update the data in place. Whereas LSM Trees just append the new data to the end of file and later do merging.
Since we are doing InPlace update, It is required/assumed that the updating do not change the data location.

There are some issues as well, Suppose during write operation the block got full, we need to split the block into 2 blocks and then update the 2 blocks and the parent block with new refrences.
During this whole operation if DB crashed, We will be in a stale sitation. Our DB will have incorrect/stale data.
How to remove this issue?
Have a WAL (Write ahead log), before doing any modification in B-Tree append the new update in this WAL. If something crashes, We can regenerate the B-Tree using this WAL.

There is a concurrency issue as well. If during write operation we have a read operation as well, How will you honor this? In LSM trees since the new data is just appended in the latest segment, we can always honor a concurrent read request from older segements and then the remerging and all can happen later.
To avoid this B-Trees uses Light weight locks.




============================
31/03/2025

Comparing B-Trees and LSM Trees
As a rule of thumb, B-Trees are read efficient and LSM Trees are write efficient. Reads are slower on LSM trees because we need to look at several SS tables to get the data.

Advantages of LSM Trees
1. B-trees write every peice of data twice, one in the WAL and then in the actual disk location.
2. Log structured indexes also do multiple writes over the lifetime. They do compaction and merging of SSTables. One write to the DB results in multiple writes over the lifetime, this is called write amplifictioa. This causes issues in SSD, as for overwrites they will wear out quickly.
3. LSM trees uses less space as we can do compaction and merging process. Furthermore we can compress the segments into smaller blocks. While in B-Trees a lot of space is wasted in fragmentation. When we need to split a page to add another key some data will be wasted in the newer pages.
4. LSM trees have higher write throughput because they just write once to the Memtable and later the other writes are async. While in B-Trees there are mutiple writes during the course of one operation.
5. Many SSDs internally uses log structured algorithms, As Log structured Algos have less fragmentation and less write throughputs, Also we have high I/O Bandwidth. 


Downside of LSM trees
1. The ongoing Compaction process can sometime interfere with the the read and write requests. Although the compaction is a async process and is completely random, but during compaction if we have a read req, it will need to wait.
2. The compaction process and the flushing of Memtable to SSTables. The first time operation when we flish the data from memtable to SS table is very fast, but Once the DB grows in size, The compaction process might slow down.
3. If the incoming write requests are much larger, and the compaction process is not frequent we will have multiple unmerged segments, This will lead to increase disk space and slower reads. 
4. Advantage of B-trees is that it has the key in only one place, but in LSM trees we have the key in multiple segemtns. For DBs which require transactional control like Relational DBs, B-Trees are much preferred as they need to put locks before write and this can be directly doable in B-Trees.




Other indexing structures
primary key - Unique, cannot be null, only one per table, used to uniquely identify a row
secondary key - Same values possible, can be null, used for indexing and joins

So far we have looked at the indexes which are used on the primary key. We can create secondary index on other keys as well. Secondary index can be created using the B-Trees or LSM Trees. Only difference is that the keys will not be unique as there can be duplicates.
This can be avoided by having a single key and references to all the entries. Or have unique key based on the row identifier.

Storing data within the index
In the index we have keys which are used for querying and the values can conatins the exact row/document. Or mostly it just contains the pointer to where the actual data is present.
This file is called the heap file where the data resides. In the cases where we have multiple indexes its not good to store entire row/document in index. Instead just store the pointer to the heap file.

When we want to update the data, we can directly update the heap file and all the indexes can refer to the same file. If the new data is larger than the old data, then we need to change the location where the new data will be written. Either can update all the indexes with the new location or we leave a forwarding pointer. 

In some situations the extra hop from index to heap file is too much, so in that cases we store the entire row/document in the Index value. These are known as clustered indexes.

Clustered indexes will lead to increase speed and performance. But this will lead to increase in size of the index. Also the writes will be slow as we need to duplicate the data to the index as well. We need to maintain consistency as well.


=========================================================

1/4/2025

Multi Column index
If we want to search based on two keys (columns) then we need to use MultiCoumn indexes. 
For ex. 
Seclect * from Person where name = 'Jaskaran' and city = 'Bangalore'

In general, In B-trees and LSM trees we have index over single column. But we can create index over multiple columns as well.
They will work but not that efficient.
There are specialized indexes which are used for these purpsoses.

Full text search and fuzzy indexing
Till now we have assumed that we will know the key that the user is searching for and we will just return the value.
But what about search engines, where we have to present all the results based on the entered word. That word can be mispelled as well so we need to accomodate that also.
Think of LSM trees and SS tables. We can look for the entered word and all the nearby words as well (sorted) and return the results.

In memory Databases
Keep everything in memory. Till now for the indexes we have seen we are using In memory storage for small operation but the data is actually stored in disk. 
With time, RAM are cheaper, so there are indexes which completely store all the data in RAM. If there is a power outage we need to rebuild this index again. For this we can use a WAL in disk or have another replica of index running on another machine. 
Ex. Memcached (In memory key value datastore)
Redis is also an example if in memory datastore.
The main advantage of in memory datastore is not that we never need to read from disk, that is possible with other indexes (LSM and B_trees) if we use much larger RAM.
The advantage is that we need to not to implement different data strcuture to write to disk. There is overhead for that.

Recently there have been studies to have more data in in-memory datastore than the available size. This is possible by removing the least used data from memory and storing that in disk, when it is accessed it is moved back to memory.


======================================================
02/4/2025

Transacation processing vs Analytics
The word transaction corresponds to a commercial transaction. Like making a sale, paying salary etc.
OLTP (Online transaction processing) - Read/write data from DB.

Transacation need not be ACID, it just means low latency read and write.

Earlier for data Analytics also we were using the same DBs. In data Analytics we need to analyze the data, and make multiple operations on the data.
This is called OLAP (Online Analytics processing)
With time for OLAP we migrate ways from Normal DBs and started using data warehouse


Data warehouse
Often in a company, we have multiple DBs. Each of these DBs are used for OLTP. We generally don't use directlty these DBs for making OLAP as these queries are expensive and may take time.
Hence we extract the data from all these DBs and dump them in a seperate read only DB.
The Business Analyst makes there queries on this DB. This DB is called a Data warehouse.

Why do we need a seperate Data warehouse?
Why can't we make OLAP on the normal DBs?

Bcz the normal DBs are not designed for such heavy analytics operations.
The normal indexes we have studied like (B-Trees and LSM trees) will not perform good for analytics queries.

Data model of Data warehouse is mostly Relational, as for analytics we generally uses SQL queries. It makes sense to use a Realtional DB for analytics as most of the data is realted. Internally data warehouse and normal Relational DBs are very different. On the surface they both uses SQL.

These days in a DB either we will have data warehousing support or OLTP support. Its very unconventional to support both.

Ex. SAP HANA, Teradata, Vertica etc.

Data modeling - Star schema, snowflake schema
We generally have a facts table in the middle, This facts table will store all the events that happens at a particular time.
For ex. all the sales made at a particalar time.

All the other tables have the foreign keys in this table. That why a name star. Fact table in the middle and all other tables around it.

In snowflake schema, we further divide the surrounded tables in smaller tables.

The Facttable is very large, it generally have 100 of columns and the rows are a lot. As for each timestamp we will have a entry.


Column Oriented Storage
In Data warehouses, we generaly query for 3,4 columns in a single query. We rarely do (Select *), So it is a waste if we use the normal DB. In normal DB, we generally have index over a particualr key and from that we query and get the rest of rows. But still we need to get all the columns from a particualr row which is a waste.

Instead Datawarehouses uses Column oriented storage. In DB they don;t store the rows, they store columns. 
Each column is stored in a DB file and when we need to get a particualr row, We just get the data from that particualr columns. One thing that is required is that the data is stored in continuos fashion in all the columns file.
By this way the processing is quite easy.
