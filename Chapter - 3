Storage and Retreival

In this chapter we will see how data is stored inside a DB.

Simplest form of DB.
DB_GET(key) returns value
DB_SET(key, value)

So DB_SET will set the key-value at the end of file.
It will not deted the previous entries of the same key.
SET operation is quite fast O(1).

DB_GET will search for the last occurence of this key and returns the corresponding value.
This operation is quite slow. We may need to search the entrie file. O(n).

We can optimize this search operation using indexes.

Index is an addiitonal data strcuture. The general idea behind them is to keep some addiitonal metadata on the side, which acts as a 
signpost and helps you to locate the data you want. An index is an additional strcuture that is derived from the primary data. 
Many DBs allows us to add and remove index, this doesn;t affect the content in DB, it only affect the performance of queries.
The read operation will become fast by using indexes but the write operations will become slow.
So its a tradeoff. 

=========================================
29/03/2025


Hash Indexes:
We know what a hash table is. For a given key we can look for a value in a hash table in O(1) time.
So for indexing Key-Value store, We can use HashTable for indexing.
Take the example where we are storing the Key-Value in a file. Each write is done at the end of a file.

Now For each write do a entry of that key in Hash Table. And for that hash key the value will be the offset/memory pointer where that data is stored. 

For Ex.

Key - Value Store
Cat - 1
Mouse - 2
Dog - 3
Cat - 4
Dog - 5

Corresponding Hash Table
Cat - 1, 4 (These are basically the offsets/memory pointers from where we can directly fetch this data)
Mouse - 2
Dog - 3,5

If you want to get the latest value, return the last value in the list
This is the scenario where we want to keep all the entries of a key in hash table.

Suppose if you only want the latest value and the past values are redundant to you.
We need not store the complete list. For ex. we are just storing the counter of how many times a key is accessed.

Cat - 4
Mouse - 2
Dog - 5

Since we are only writing to a disk, We can run out of disk space. How do we prevent that?
So don't write in a single file, When one file reach a certain size, start writing in other files.
Then perform a compaction on all the files. Meaning merge the file and keep only the latest entry.

This compaction can be an async process. Once the compaction is done we are safe to delete the old files and just read from the new one.

Each segment (file) has its own hashmap. So when we need to look for a key, we first check the most recent segment hashmap, then the second most recent and so on.
Since compaction and merging keep the no. of segments on lower side.

BTW this hashmap is in memory. Using system RAM.
Bitcask is a very popular Key-Value store that works well with just this basic approach.

1. Deleting a record - For deleting a record we add a flag in the file/segment, When we do merge, We discard the previous key from hastable and the new segment entry.
2. Crash recovery - If the server restart the in memory data store is lost. We can normally traverse the all segemnts and generate the hashmaps again but that is very slow. Instead we can keep the snapshots of the hashmaps and store them in datadisk.
3. Partially written records - If the system crashes midway, Bitcask have checksums to detect that and rollback.
4. Concurrency control - For writing we will have a single write thread but for read we can have multiple. So write will happen in a sequential manner but for reading multiple threads can access at the same time.

An append only design seems like a waste. Why don;t we update the values instead of keep appending and then doing merging and maintaining seperate hashmaps for each segment. 
Well there are advantages.
1. Appending and segment merging are sequential write operations, which are much faster than random writes.
2. Concurrency and crash recovery are much simpler if segment files are append only. Bcz suppose during a write a crash happened. If we have random writes we can be in a state where we are in between the old and new data.
3. Merging old files avoids the data fragmentation problem.

Well there are some issues as well in using HashMap index
1. Since its in memory we are limited. Once the RAM is full, we cannot do anything. We can use a in memory hashMap but that is not fast enough.
2. Range queries are not efficient. We cannot look up all the keys between certain size (1000 - 9000). We need to go over all the keys one by one.


=================================
30/March/2025

SS Tables (Sorted string table)
In the normal log segents with hash indexing, we were just appending the key-value pairs at the end of the file. The older values of the key takes precedence. Otherwise it doesn;t matter how the other keys are arranged.

If while writing we sort the key value pairs based on the key, they are called SS tables. We also require that each key only appears once in the merges segment, that is already ensured by the compaction processs.

How is this helpful over the normal log segments with hash indexing?

1. Merging segment is very easy. We will use the concept of merge sort. We will take the most recent and the second most recent segment and merge them. The merged segment is also sorted.
What if 2 segments have the same key?
Its simple, choose the key-value which is most recent (i.e. from the latest segment).

2. We not don't need to index each key. Why? We can index some keys and then from those keys we can get to our target key since the file is sorted.

For ex. 
Index
HA1 - 0x7489
HA5 - 0x8780
HA9 - 0xiue2

Suppose you need to find HA7, Since you know the offset of HA5 and HA9, just scan between these keys in the SS Table.

This prevent memory exhausation since very few keys are indexed.

3. Since for read requests we need to scan over multiple keys, We can divide the SS (sorted segment) tables in multiple blocks.

For ex. All the keys from HA1-HA4 in Block1. All the keys from HA5-HA8 in other block.
We can compress these blocks and save them on disk. Now the index will point to the offset of these blocks.
This prevents space and network I/O.


Constructing and Maintaining SS Tables
How can we get the Key-Value pairs to be in sorted order?
maintaining a sorted structure in disk is possible (B-Tress)
maintaining a sorted structure in memory is very easy.
We can use a Tree data structure like Red Black trees or AVL trees. In these trees we can insert the data in any order but we will read them in sorted order.

******  VERY IMPORTANT START  *********
We can make our storage engine work as follows:
1. Have an In memory Tree Data structure like AVL tree, Write the data to this tree. Since this is in memory we can call it memtable.
2. When the memtable reaches a certain size write the data on disk as a SS table file. This new SS table becomes the most recent segment.
3. For reading we will first try to find the key in the memtable, then the most recent segment, then the second most.
4. From time to time run a merging process to merge these segments. With this we can remove the deleted keys and overwritten keys.


This suffers from only one problem, In case of a crash the data that is present in memtable will be lost, as it is still not written to SS table.
To remove this problem, just have a log file which will append all the data in the end. It will just write the data in memtable.
Once the memtable data is written to SS table, we can discard this log. This is just for crash purposes.

******  VERY IMPORTANT END  *********

Above algorithm is used in LevelDb and RockDB. Storage engines in Cassandra and HBase are also inspired by this. These type of storage engines are called LSM engines (Log structured merge tree).

Performance optimizations:
When a key does not exist, we need a lot of read operations.
We first need to check the Memtable, then the various segments. Only then we can be sure that this key does not exist.
This is very long operation. To optimize this we can use Bloom Filters. (It is an in memory datastore that can efficiently tell if a key exist in DB or not.)




B-Trees
Most relational DBs uses B-Trees for indexing. How is it different from the log based indexing?
Well in log based indexing we have a segments of variable sizes, with each segment having a index (hash table). 
But in Btrees we have a fixed size block. In these blocks we store key value pairs.
One block will have ref to other blocks. The value for a particular key will be at the leaf node.

Root Block:
ref, 100 - ref, 150 - ref, 200 - ref |  This block will lead us to data between keys 100 <= key < 200

ref , 111 - ref, 127 - ref, 140- ref | So the first ref in the root block lead us to this

This way we need to traverse and we will get to the jey we are looking for.
O(Logn) for search/ Update.
For adding new key search the particualr block, if there is space insert the key.
If not divide that block into 2 full sized block update the ref in parent block and do addition.

B-Trees are quite reliable and has been time proven.

Most fundamental difference between B-Trees and Log based LSM Trees is that B-Trees update the data in place. Whereas LSM Trees just append the new data to the end of file and later do merging.
Since we are doing InPlace update, It is required/assumed that the updating do not change the data location.

There are some issues as well, Suppose during write operation the block got full, we need to split the block into 2 blocks and then update the 2 blocks and the parent block with new refrences.
During this whole operation if DB crashed, We will be in a stale sitation. Our DB will have incorrect/stale data.
How to remove this issue?
Have a WAL (Write ahead log), before doing any modification in B-Tree append the new update in this WAL. If something crashes, We can regenerate the B-Tree using this WAL.

There is a concurrency issue as well. If during write operation we have a read operation as well, How will you honor this? In LSM trees since the new data is just appended in the latest segment, we can always honor a concurrent read request from older segements and then the remerging and all can happen later.
To avoid this B-Trees uses Light weight locks.
