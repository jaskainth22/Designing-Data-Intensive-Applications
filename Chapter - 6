Partitioning

IN Replication  we sturdies how the same data is copied over multiple nodes, so that we have high availabiltuy, fault tolerance and high read or write thorughput.
In Partitioning, we divide our large dataset into smaller datasets and these datasets are present on nodes. All these nodes works independetly. If we want to fetch some speicifc data , we need to reach that node. This is done for scalability. If our dataset it too large it will be hard to handle by a single node.

Partioning is combined with Replication. The data is divided in multiple datasets and stored on multiple node. THen we store the copies of these datasets on other nodes. This way we have same dataset in multiple nodes.

Each node can store multiple partitions. We will have leader for each partiotin and followers. These are distributed across multiple nodes.
Each node acts as a leader for some partitions and followers for others.

Partiotioning of a key value data
How to make sure that the data is evenly spread across partioions?
Ideally if we think, we distribute data across 10 nodes, we should be able to have 10 times more storage and 10 times more read/write throughput. But sometimes this does not happen. All the load falls on a small set of partiotions. These are called skewed partions, When the data is not evenly spread. The partition on which we have high load is called a hotspot.

So how to resolve this? We can randomly write data to different partiotions. Yes this will solve this issue, but how will you know which data is present in which partioion? You need to make read requests to all the partitions in parallel.

1. Partiotioning by Key Range
    We divide the data based on the keys. For ex. All the keys starting from A-D in P1, E-L in P2, M-Z in P3. If we know these partitoin ranges and also know which partioton is stored in which node we can easily make the read requests.

    In this type of partiotioning, we may have situations in which some partiotion have more data and some partiotions have less data. If we need to evenly distribute data across partiotins we need to adjust the partiotn boundaries accordingly.
    Within a partition the data is stored in a sorted format (using SS tables and LSM trees). With this we can easily make range queries.
    However the downside of key based partiuoning is hotspots. There might be chances that only one node is doing all the work. How?
    Consider a DB, that writes data based on timestamp as key. So we partioin based on timestamps. So all the writes for today, will end going to the same node and that node will be a hotspot. To avoid this problem, we need to use some other thing as key instead of timestamp. Like what? We can have sensor name as primary key followed by timestamp,

2. Partiotining by hash of key
    We should have storing has functions. We take hash of the keys and then put them in different partiotins. Each partiotion is assigned some hash ranges.
    For ex. Our hash function returns 0-1000000. These ranges will be distirbuted across partiotions evenly.
    When we take hash of a key, We will put that key in that partition. With this way keys are evenly distributed across partiotns.
    If the ranges are not evenly distributed this is something similar to Consistent Hashing. In consistent hashing we choose random partition boundaries to avoid the need for central control. Consistent hashing does not work very well with databases. We will see later why.

    The con here is that we lose the sorted order of keys. Earlier the keys were sorted and in the same partition, we were able to do range queries.
But niw this is not possible. If we do a range query in MongoDB, we need to go to multiple partition. Range queries are not supported in Riak, CouchBase or Voldemort.

Skewed Workloads and Relieving hotspots
hashing the key removed the skew and hotspot issue, but does not resolve it completely. If the read and write are in the saome key, we have the same issue.
For ex. Consider a celebrity, If they post something, all the reads and wirtes will be for that celeb key, it will go to the same partiotin. Now it on the application code to resolve this issue, If they see a key is a hotspot, they should add some random digit at the end, this will lead to splitting the data across multiple keys and hence partiotions. But still this adds overhead, hence not recommended.


Partitioning and Secondary indexes
In our key value data model, If we are doing partitioning just on the primary key its very easy. We can directlty follow that primary key for our read and write operations. Issues arises when we have secondary indexes. 
Secondary index doesn't identify a record uniquely but rather is a way of searching for occurences of a particular value.


1. Partiotioning Secondary indexes based on document
Based on the documentId/ primary key we partiotined the DB. Each Partiotion maintains its own secondary index. Suppose we have a DB conataining car info. Keys from 0-500 in P1, 500-1000 in P2. It has two secondary indexes on color and brand.
Each partition maintains its own secindary index. Writing is easy in this case. Just add the entry in the correct partition and there secondary indexes.
Reading is difficult. Suppose we need to search for all the casrs with a particaulr color, we need to send the requests to each partition. This can lead to latency issues.

2. Partiotioning secondary indexes based on terms
Instead of having a seperate secondary index in each partition, we will have a single secondary index mapping, called the global index. It will contains entries from all the partiotions. 
This make read requests based on a secondary index very easy. The request now just need to go to the secondary index.
However this secondary index can grow in size, and we should partition it into multiple smaller partitions and store that on seperate nodes.
Partitioning the secondary index can be done based on key or hash. For ex. supppose we have secondary index based on color, So all colors from A-P goes in one partition, all the remaining goes in another partition. This can help in range queries. However can suffer from skewed and hotspot problem. We can do hash based partitioning but then will lose the range based support.
Writes are difficult in this approach, because the main write can go to some partiton and the secondary index may lie in some other partition.


Rebalancing Partitions
With time, We need to add More CPUs to handle query throughput. We need to add more disks to handle dataset size increase. A machine may fail.
Due to all of this we need to transfer load frmo one node to another. This process is called rebalancing.

Rebalancing should follow some minimum requirements:
1. After rebalancing the load should be fairly distributed between the nodes.
2. while rebalancing is happening, DB should keep accepting read and write requests.
3. No more data then necessary should be moved between the disks.


Strategies for rebalacning

1. Based on hash key
We can think that each node should contains data from some specific key range only. For ex. N1 conatins keys from 0-250, N2 contains keys from 251-500. Whenever a key comes we can do hash(key)%N. Where N is the number of nodes. Well this works fine, but if the number of nodes increases or decreases, keys needs to be shifted among the partitions, this is too much of an overhead. Suppose earlier for key x, we were going to node 0 ,now we have to go to node 1. SO this mapping gets changed. This is overkill and bad design.

2. fixed number of partitions
Create many more partiotns than there are nodes and assign several partitions to each node. For ex. a DB running on a cluster of 10 nodes may be split into 1000 partitions, so that approx 100 partitions are assigned to each node.
If a node is added, some partitions can be moved from each node to this new node, If a node is removed, partitions are put in other nodes.

Since only entire partitions are moved between nodes, key assignment does not change.
This type of rebalancing is used in Riak, Elasticsearch, Couchbase, Voldemort.

In this config, the number of partitions stays fixed. 

3. Dynamic partitioning
For DBs that uses key range partitioning, a fixed number of partitions with fixed boundaries, can cause issues. There maybe a scenario where all the data may end up in a single partition. SO in that case we use dynamic partitioning, if the data exceeds certain threshold, that partiotion is split in 2 new partitoons, each containing half of the data.
If a lot of data is deleted from a partition, it can be merged with adjacent partitions.
Each partition is assigned to one node, and each node can handle multiple partitions. When a partition is split in 2 partitions, 1 partiton is assigned to a new node.
Dynamic partitioning is helpful both in key range and hash based partitioning. 
The only caveat here is that, initially, there will only be one partition, and as the size grows, the partitions are created based on keys. SO initially all the work is done by only one node and other nodes stays idle.


====  CHATGPT on Rebalancing partitions========

Summary:
Rebalancing partitions means redistributing data and workload across nodes when a database’s conditions change (e.g., added nodes, node failures, increased data).
Rebalancing should:

1. Distribute load fairly,
2. Continue operations during rebalancing,
3. Minimize the amount of data moved.

There are different strategies for partitioning and rebalancing:

1. Hash mod N (bad idea): If you hash keys with mod N, changing the number of nodes causes almost all keys to move, making rebalancing expensive.

2. Fixed number of partitions: Predefine many partitions (e.g., 1,000), assign them to nodes. When nodes change, partitions are reassigned without redefining how keys map to partitions. Used by databases like Riak, Couchbase, and Elasticsearch.

3. Dynamic partitioning: Partitions are split or merged based on size dynamically. Used by systems like HBase and MongoDB. Good for unpredictable data growth but initially puts all load on one node unless pre-splitting is done.

4. Partitioning proportionally to nodes: Systems like Cassandra create a fixed number of partitions per node. When nodes are added, existing partitions are split and shared. Helps keep partition sizes stable relative to total data size.

Explanation:
When running a distributed database, data is partitioned (split up) across multiple nodes. Over time, the system changes—more users, more data, or hardware changes—so you need to rebalance: move data around to keep the load evenly shared.

Challenges during rebalancing:

1. Fairness: Every node should carry a similar amount of work.
2. Availability: Users should still be able to read and write.
3. Efficiency: Move as little data as possible to avoid expensive network and disk usage.

Why hash mod N is bad:
Using hash(key) % N ties your data directly to the number of nodes (N). So if N changes, almost all the keys move, causing massive data reshuffling. That's slow, costly, and dangerous for uptime.

Better options:

1. Fixed number of partitions:
Pick a large number of partitions from the start (e.g., 1,000).
Each node gets several partitions.
If a new node joins, just move some partitions to it.
Good for: Predictable growth and relatively steady hardware.

2. Dynamic partitioning:
Start with a few partitions and split them as data grows.
Merge partitions if data shrinks.
HBase and MongoDB use this style.
Good for: Unpredictable or huge growth.
Problem: Initially, a small dataset can overload a single node unless you "pre-split" ahead of time.

3. Partitions proportional to nodes (Cassandra's way):
Each node gets a fixed number of partitions.
When a new node comes, it randomly splits some partitions and takes half the data.
Helps maintain a steady partition size even as more nodes or more data come in.
Based on consistent hashing principles.

=================

Automatic or manual rebalancing
So who should do the rebalacning work? Should this be done automatically when the partition size increases or nodes are added removed, or this should be done by a Human?
Well a human interaction should be there, system should detect that the nodes are getting full and needs rebalacning but in the end a human interaction should be required, else system can automatically choose to do this.
Rebalancing is an expensive operation. During the rebalacning the read/write requests can take a lot of time. So suppose if a node is overloaded and taking time to respond, other nodes mught think that it is dead and may start an rebalcning opertaion. But this will lead to further stress on the node.

Request Routing
How does we know where does a key recide? We know the partition in which key is present (either through key based partitioning or hash based partitioning). But we dont know the node in which the partition recides. 
There can be 3 approaches,
1. Send request to any arbitary node, if it has the partiotin/key, Good. Else it will send the request to appropriate node.
2. Have a middleware service, which should redirect the read request to right node.
3. Client should have knowledge of this.

We can have a zookeeper kind of thing, which should maintain the key mapping to partitons and further partiotin mapping to nodes.
This zookeeper should update itself, whenever rebalancing happens. Zookeeper can be given to the client/ middleware service or each node.









